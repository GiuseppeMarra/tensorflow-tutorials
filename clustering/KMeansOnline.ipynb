{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {
    "collapsed": true
   },
   "level": 1,
   "source": [
    "KMeans online using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is a very simple but powerful clustering algorithm. In its native form, the algorithm is very easy to understand:\n",
    "1. Initialize randomly k points (called centroids  or centers) in the same multidimensional space of the input\n",
    "2. Assign each of the input points to its closest center\n",
    "3. Update each center with the mean of the points assigned to it.\n",
    "4. Repeat 2-3 until convergence (i.e. the assignment to the clusters remains unchanged)\n",
    "\n",
    "In this tutorial, we will provide the implementation of a clustering algorithm that is inspired by k-means but it can be implemented inside a gradient descent scheme, allowing us to incorporate this algorithm inside a general TensorFlow graph.\n",
    "\n",
    "The method works as following:\n",
    "1. Initialize randomly k variables (the centers)\n",
    "2. Assign each of the batch points to its closest center\n",
    "3. Minimize the average distance between each point and its assigned center\n",
    "\n",
    "This method is teoretically sound, since it can be shown that a gradient descent scheme performs updates of the centers that are an approximation of the mean of the assigned points.\n",
    "\n",
    "For this implementation, we will use `sklearn` library to automatically construct a dataset for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as dt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `D` be the dimension of the input space, `N` the number of the input points and `K` the number of centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "N = 100\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the dataset and convert the input into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = dt.make_blobs(centers=K, n_features=D, n_samples=N)\n",
    "X = tf.to_float(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the centers' `Variable`s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = tf.Variable(initial_value=tf.truncated_normal(shape=[K, D]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compute the distance between each element of `X` and each element of `M`. To this end we will expand the dimensions of both the tensors and then we evaluate the distance by: _(i)_ computing the component wise difference, _(ii)_ computing the square of these differences, and _(iii)_ reducing the component dimension by summing up squared differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xex = tf.tile(tf.expand_dims(X, axis=1), [1,K,1])\n",
    "Mex = tf.tile(tf.expand_dims(M, axis=0), [N,1,1])\n",
    "dist = tf.reduce_sum(tf.square(Xex - Mex), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will compute the cluster assignment (` argmin` over distances) and convert it into a one-hot representation `R` (1 for closest cluster, 0 elsewhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.argmin(dist, axis=1)\n",
    "R = tf.one_hot(y, depth=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to define our objective function. Straightforwarly, it is the sum of all the distances of each data point to its assigned cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.multiply(R, dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the training operation using ` AdamOptimizer` for the minimization of `cost`. Then, we will train the model using a fixed number of iterations (just standard Tensorflow learning code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):\n",
    "        _ = sess.run((train_op))\n",
    "    Y = sess.run(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the learned clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(K):\n",
    "    plt.scatter(inputs[Y==i, 0], inputs[Y==i, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}