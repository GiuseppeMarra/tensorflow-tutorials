{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Char-Word-2-Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows an implementation of the standard Word2Vec algorithm with the variant of using characters as interface with the model (i.e. for both input and supervision).\n",
    "\n",
    "This feature brings several advantages, two of whom worth mentioning:\n",
    "1. it allows avoiding the use of a dictionary of words, often not avialable or too big;\n",
    "2. exploiting characters allows the capture of morphological similarities between words (which, even for humans, is the first signal of the meaning of a word, especially if unknown).\n",
    "\n",
    "In order to use characters, this model applies two main variants to the standard model (we will use the CBOW model):\n",
    "1. the lookup table, which stores the embeddings of words, is substituted by a recurrent neural network over characters; in this way, the goal of the model is to learn the weights of this NN and not the lookup table;\n",
    "2. the softmax layer over words is substituted with a decoder into charactesrs and a softmax layer over characters (by far less computatianlly expensive, making unnecessary the use of approximations like NCE)\n",
    "\n",
    "The entire code to build and run the model is provided in the file `CharWord2Vec.py`. Here, we provide the description of how to build the model.\n",
    "\n",
    "Using a pattern often exploited in Google TensorFlow tutorials, we divide the model into two classes:\n",
    "1. a `Config` class, which stores all the hyperparameters of the model and some useful variables;\n",
    "2. a `Model`class, which builds the structure of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import rnn, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "\n",
    "    char_vocab_size = 256  # The size of the character vocabulary\n",
    "    char_embed_size = 50 # The size of the character embedding\n",
    "    window_size = 11  # The total size of the window [window//2 center window//2]\n",
    "    if window_size % 2 == 0:   window_size += 1  # Assuring window_size is odd\n",
    "    context_size = (window_size - 1)\n",
    "    batch_num_words = 1024 # We use a online built unique set of words, in order not to repeat embeddings computation \n",
    "    batch_size = batch_num_words - context_size\n",
    "    cell_type = \"LSTM\"\n",
    "    encoder_rnn_size = 500  # hidden layer of the encoder RNN\n",
    "    decoder_rnn_size = 1000 # hidden layer of the decoder RNN\n",
    "    encoder_num_layers = 1\n",
    "    embedding_size = None\n",
    "    word_max_len = 15\n",
    "    _PAD = 0\n",
    "    _GO = 1\n",
    "    _EOW = 2\n",
    "    _UNK = 3\n",
    "    shuffling = False\n",
    "    learning_rate = 0.001\n",
    "    beta = 0.000\n",
    "    dropout = 0.5\n",
    "    epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "\n",
    "    def __init__(self, config, is_training, is_eval=False):\n",
    "\n",
    "        self.x = tf.placeholder(dtype=tf.int32, shape=[None, config.word_max_len])\n",
    "        chars_length = tf.reduce_sum(tf.where(self.x>0, tf.ones_like(self.x), tf.zeros_like(self.x)), axis=1)\n",
    "\n",
    "        self.y = self.x[(config.context_size//2):-(config.context_size//2),:]\n",
    "\n",
    "        '''Characters Embeddings'''\n",
    "        embedding = tf.get_variable(\"embedding\", [config.char_vocab_size, config.char_embed_size], dtype=tf.float32)\n",
    "        chars = tf.nn.embedding_lookup(embedding, self.x)\n",
    "\n",
    "        '''Words Embeddings'''\n",
    "        self.output = rnn.morph_encoder(chars, chars_length, config.encoder_rnn_size, is_training=is_training)\n",
    "        self.word_embedding = self.output\n",
    "\n",
    "        if is_eval:\n",
    "            return\n",
    "\n",
    "        '''Contexts Embeddings'''\n",
    "        indices = data.create_indices_for_context2vec(config.window_size, config.batch_num_words, skip_center=True)\n",
    "        indices = tf.cast(indices, tf.int32)\n",
    "        words_contexts = tf.gather(self.output, indices=indices, name=\"gather1\")\n",
    "        self.contexts = tf.reduce_sum(words_contexts, axis = 1)\n",
    "\n",
    "        \"Deconding Phase\"\n",
    "        labels = self.y[:,1:]\n",
    "        weights = tf.where(labels > 0, tf.ones_like(labels), tf.zeros_like(labels))\n",
    "        logits = rnn.deconding(initial_state=self.contexts,\n",
    "                               y=self.y,\n",
    "                               decod_size=config.decoder_rnn_size,\n",
    "                               embedding=embedding,\n",
    "                               vocab_size=config.char_vocab_size,\n",
    "                               batch_size=config.batch_size,\n",
    "                               word_max_len=config.word_max_len,\n",
    "                               dropout=config.dropout,\n",
    "                               is_training=is_training)\n",
    "\n",
    "        \"Loss\"\n",
    "        self.predictions = tf.argmax(logits,  axis=2)\n",
    "        weights = tf.cast(weights, tf.float32)\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            labels,\n",
    "            weights,\n",
    "            average_across_timesteps=True,\n",
    "            average_across_batch=True\n",
    "        )\n",
    "\n",
    "        self.cost = loss\n",
    "        if not is_training:\n",
    "            return\n",
    "        self.train_op = tf.train.AdamOptimizer(config.learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Model` class, we exploit two utility functions defined in the utils script `rnn.py`:\n",
    "1. `rnn.morph_encoder`, which is the RNN for computing the word embedding using characters\n",
    "2. `rnn.deconding`, which is the RNN for decoding the sequence of characters of the target word given a vectorial representation of the context around it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell(size, type, dropout=None, proj=None, is_training = True):\n",
    "    cell = None\n",
    "    if type == \"LSTM\":\n",
    "        cell= tf.contrib.rnn.BasicLSTMCell(size)\n",
    "    elif type == \"GRU\":\n",
    "        cell= tf.contrib.rnn.GRUCell(size)\n",
    "    if dropout is not None and is_training:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=dropout, output_keep_prob=1.0, state_keep_prob=1.0)\n",
    "    if proj:\n",
    "        cell = tf.contrib.rnn.OutputProjectionWrapper(cell, proj)\n",
    "    return cell\n",
    "\n",
    "def morph_encoder(chars, chars_length, size, cell_type=\"LSTM\", dropout=None, is_training=True):\n",
    "    '''Here we take a batch of words and compute their morphological embeddings, i.e. a hidden representation\n",
    "    of a RNN over their characters'''\n",
    "    with tf.variable_scope(\"MorphologicEncoder\"):\n",
    "        with tf.variable_scope(\"fw\"):\n",
    "            char_rnn_cell_fw = cell(size, cell_type, dropout, is_training=is_training)\n",
    "        with tf.variable_scope(\"bw\"):\n",
    "            char_rnn_cell_bw = cell(size, cell_type, dropout, is_training=is_training)\n",
    "        _, (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw=char_rnn_cell_fw,\n",
    "                                                               cell_bw=char_rnn_cell_bw,\n",
    "                                                               inputs=chars,\n",
    "                                                               sequence_length=chars_length,\n",
    "                                                               dtype=tf.float32)\n",
    "\n",
    "    return  tf.concat((fw_state.h, bw_state.h), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconding(initial_state,decod_size, vocab_size, embedding, y, word_max_len, batch_size, cell_type=\"LSTM\", dropout=1.0, is_training=True):\n",
    "    '''Decoding Phase'''\n",
    "    decoder_input = initial_state\n",
    "    initial_decoder_state = tf.contrib.rnn.LSTMStateTuple(c=tf.zeros_like(decoder_input),\n",
    "                                                          h=decoder_input)\n",
    "    decoder_cell_fw = cell(decod_size, cell_type, dropout=dropout, is_training = is_training)\n",
    "\n",
    "    decoder_inputs = tf.nn.embedding_lookup(embedding, y[:, :(word_max_len - 1)])\n",
    "    decoder_inputs = tf.unstack(decoder_inputs, axis=1)\n",
    "    final_outputs, _ = tf.nn.static_rnn(cell=decoder_cell_fw,\n",
    "                                        dtype=tf.float32,\n",
    "                                        inputs=decoder_inputs,\n",
    "                                        initial_state=initial_decoder_state)\n",
    "\n",
    "    W = tf.get_variable(\"softmax_w\", [decod_size, vocab_size], dtype=tf.float32)\n",
    "    b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "\n",
    "    output = tf.reshape(tf.stack(axis=1, values=final_outputs), [-1, decod_size])\n",
    "\n",
    "    logits = tf.matmul(output, W) + b\n",
    "    logits = tf.reshape(logits, [batch_size, word_max_len - 1, vocab_size])\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}